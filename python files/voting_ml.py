# -*- coding: utf-8 -*-
"""A33 Voting ML 4B Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1emns9ChC2L0dXcvhp01PTnJGqElCmUKL
"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score
import numpy as np
import warnings

from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean_no_outliers_IQR.csv')

df.info()

non_zero_variance_features = df.loc[:, df.var() > 0]
X = non_zero_variance_features.drop(columns=['targets'])
y = df['targets']

# prompt: BEST random state
#  from 0 to 1000

best_accuracy = 0
best_random_state = 0

for random_state in range(1001):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)

    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_random_state = random_state

print(f"Best random state: {best_random_state}")
print(f"Best accuracy: {best_accuracy}")

from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import warnings

warnings.filterwarnings("ignore")


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=best_random_state)

# Random Forest
rf_model = RandomForestClassifier(random_state=best_random_state)
rf_model.fit(X_train, y_train)
rf_train_score = rf_model.score(X_train, y_train)
rf_test_score = rf_model.score(X_test, y_test)
rf_cv_score = cross_val_score(rf_model, X, y, cv=20).mean()
print("Random Forest:")
print("Training Accuracy:", rf_train_score)
print("Testing Accuracy:", rf_test_score)
print("Cross-Validation Accuracy:", rf_cv_score)

# K-Nearest Neighbors
# K-Nearest Neighbors
best_k = 0
best_knn_accuracy = 0

for k in range(1, 100):  # Try k values from 1 to 20
  knn_model = KNeighborsClassifier(n_neighbors=k)
  knn_model.fit(X_train, y_train)
  knn_train_score = knn_model.score(X_train, y_train)
  knn_test_score = knn_model.score(X_test, y_test)
  knn_cv_score = cross_val_score(knn_model, X, y, cv=20).mean()
  knn_avg_accuracy = (knn_cv_score + knn_train_score + knn_test_score) / 3

  if knn_avg_accuracy > best_knn_accuracy:
      best_knn_accuracy = knn_avg_accuracy
      best_k = k


print("\nK-Nearest Neighbors:")
print("Best k:", best_k)
print("Training Accuracy:", knn_train_score)
print("Testing Accuracy:", knn_test_score)
print("Cross-Validation Accuracy:", knn_cv_score)

# Logistic Regression
lr_model = LogisticRegression(random_state=best_random_state)
lr_model.fit(X_train, y_train)
lr_train_score = lr_model.score(X_train, y_train)
lr_test_score = lr_model.score(X_test, y_test)
lr_cv_score = cross_val_score(lr_model, X, y, cv=20).mean()
print("\nLogistic Regression:")
print("Training Accuracy:", lr_train_score)
print("Testing Accuracy:", lr_test_score)
print("Cross-Validation Accuracy:", lr_cv_score)

# Support Vector Machine
svm_model = SVC(random_state=best_random_state)
svm_model.fit(X_train, y_train)
svm_train_score = svm_model.score(X_train, y_train)
svm_test_score = svm_model.score(X_test, y_test)
svm_cv_score = cross_val_score(svm_model, X, y, cv=20).mean()
print("\nSupport Vector Machine:")
print("Training Accuracy:", svm_train_score)
print("Testing Accuracy:", svm_test_score)
print("Cross-Validation Accuracy:", svm_cv_score)

xgboost_model = XGBClassifier(random_state=best_random_state)
xgboost_model.fit(X_train, y_train)
xgboost_train_score = xgboost_model.score(X_train, y_train)
xgboost_test_score = xgboost_model.score(X_test, y_test)
xgboost_cv_score = cross_val_score(xgboost_model, X, y, cv=20).mean()
print("\nXGBoost:")
print("Training Accuracy:", xgboost_train_score)
print("Testing Accuracy:", xgboost_test_score)
print("Cross-Validation Accuracy:", xgboost_cv_score)

# prompt: select best model from above
rf_avg_acc = (rf_cv_score+rf_train_score+rf_test_score)/3
knn_avg_acc = (knn_cv_score+knn_train_score+knn_test_score)/3
lr_avg_acc = (lr_cv_score+lr_train_score+lr_test_score)/3
svm_avg_acc = (svm_cv_score+svm_train_score+svm_test_score)/3
xgboost_avg_acc = (xgboost_cv_score+xgboost_train_score+xgboost_test_score)/3


# Create a dictionary to store the model names and their cross-validation scores
model_scores = {
    "Random Forest": rf_avg_acc,
    "K-Nearest Neighbors": knn_avg_acc,
    "Logistic Regression": lr_avg_acc,
    "Support Vector Machine": svm_avg_acc,
    "XGBoost": xgboost_avg_acc
}

# Find the model with the highest cross-validation score
best_model = max(model_scores, key=model_scores.get)
best_score = model_scores[best_model]

print("\nBest Model:", best_model)
print("Best Average Accuracy:", best_score)

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import numpy as np
import warnings

warnings.filterwarnings("ignore")

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=best_random_state)

# Create individual models with default parameters (random_state set for reproducibility)
rf_model = RandomForestClassifier(random_state=best_random_state)
knn_model = KNeighborsClassifier()
lr_model = LogisticRegression(random_state=best_random_state)
svm_model = SVC(probability=True, random_state=best_random_state)
xgboost_model = XGBClassifier(random_state=best_random_state)

# Define hyperparameter grids for hard and soft voting
hard_voting_params = {
    'voting': ['hard'],
    'weights': [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [1, 2, 1, 1, 1], [1, 1, 2, 1, 1], [1, 1, 1, 2, 1], [1, 1, 1, 1, 2]]
}
soft_voting_params = {
    'voting': ['soft'],
    'weights': [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [1, 2, 1, 1, 1], [1, 1, 2, 1, 1], [1, 1, 1, 2, 1], [1, 1, 1, 1, 2]]
}

# Perform grid search for hard voting
hard_voting_model = GridSearchCV(VotingClassifier(estimators=[('rf', rf_model), ('knn', knn_model), ('lr', lr_model), ('svm', svm_model), ('xgb', xgboost_model)]),
                                 hard_voting_params, cv=20)
hard_voting_model.fit(X_train, y_train)

# Perform grid search for soft voting
soft_voting_model = GridSearchCV(VotingClassifier(estimators=[('rf', rf_model), ('knn', knn_model), ('lr', lr_model), ('svm', svm_model), ('xgb', xgboost_model)]),
                                 soft_voting_params, cv=20)
soft_voting_model.fit(X_train, y_train)


# Print the best hyperparameters for each model
print("Best Hyperparameters for Hard Voting:")
print(hard_voting_model.best_params_)
print("\nBest Hyperparameters for Soft Voting:")
print(soft_voting_model.best_params_)

hard_voting_train_score = hard_voting_model.score(X_train, y_train)
hard_voting_test_score = hard_voting_model.score(X_test, y_test)

soft_voting_train_score = soft_voting_model.score(X_train, y_train)
soft_voting_test_score = soft_voting_model.score(X_test, y_test)

print("\nHard Voting Training Accuracy:", hard_voting_train_score)
print("Hard Voting Testing Accuracy:", hard_voting_test_score)

print("\nSoft Voting Training Accuracy:", soft_voting_train_score)
print("Soft Voting Testing Accuracy:", soft_voting_test_score)

# Evaluate both hard and soft voting using 20-fold cross-validation
hard_voting_cv_score = cross_val_score(hard_voting_model.best_estimator_, X, y, cv=20).mean()
soft_voting_cv_score = cross_val_score(soft_voting_model.best_estimator_, X, y, cv=20).mean()

print("Hard Voting Cross-Validation Accuracy:", hard_voting_cv_score)
print("Soft Voting Cross-Validation Accuracy:", soft_voting_cv_score)

# Assign the best performing model to clf
if soft_voting_cv_score > hard_voting_cv_score:
    clf = soft_voting_model.best_estimator_
    print("\nSoft voting performs better.")
else:
    clf = hard_voting_model.best_estimator_
    print("\nHard voting performs better.")

print("\nBest performing model configuration:", clf)


y_train_pred = clf.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Train Set Accuracy: {train_accuracy:.4f}")

# Evaluate the best model on the test set
y_test_pred = clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Set Accuracy: {test_accuracy:.4f}")

# Perform 20-fold cross-validation on the best model
cv_scores = cross_val_score(clf, X, y, cv=20)
print("Cross-Validation Scores: ", cv_scores)

# Average accuracy of cross-validation
average_accuracy = np.mean(cv_scores)
print("Average Accuracy of Cross-Validation: ", average_accuracy)

# Print the best hyperparameters for each model
print("Best Hyperparameters for Hard Voting:")
print(hard_voting_model.best_params_)
print("\nBest Hyperparameters for Soft Voting:")
print(soft_voting_model.best_params_)

hard_voting_train_score = hard_voting_model.score(X_train, y_train)
hard_voting_test_score = hard_voting_model.score(X_test, y_test)

soft_voting_train_score = soft_voting_model.score(X_train, y_train)
soft_voting_test_score = soft_voting_model.score(X_test, y_test)

print("\nHard Voting Training Accuracy:", hard_voting_train_score)
print("Hard Voting Testing Accuracy:", hard_voting_test_score)

print("\nSoft Voting Training Accuracy:", soft_voting_train_score)
print("Soft Voting Testing Accuracy:", soft_voting_test_score)

# prompt: make loop to select best k feature also print avg cv accuracy for all features

import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score
import numpy as np
import warnings
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, cross_val_score


best_k_features = 0
best_avg_cv_accuracy = 0

for k in range(1, X.shape[1] + 1):
    # Apply feature selection using chi-square
    select_k_best = SelectKBest(score_func=chi2, k=k)
    X_selected = select_k_best.fit_transform(X, y)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)

    # Hard Voting
    hard_voting_model = GridSearchCV(VotingClassifier(estimators=[('rf', rf_model), ('knn', knn_model), ('lr', lr_model), ('svm', svm_model), ('xgb', xgboost_model)]),
                                 hard_voting_params, cv=20) # Assuming hard_voting_params are defined elsewhere
    hard_voting_model.fit(X_train, y_train)
    hard_voting_test_score = hard_voting_model.best_estimator_.score(X_test, y_test)
    hard_voting_train_score = hard_voting_model.best_estimator_.score(X_train, y_train)
    hard_voting_cv_scores = cross_val_score(hard_voting_model.best_estimator_, X_test, y_test, cv=20)
    avg_hard_voting_cv_score = np.mean(hard_voting_cv_scores)

    #Soft Voting
    soft_voting_model = GridSearchCV(VotingClassifier(estimators=[('rf', rf_model), ('knn', knn_model), ('lr', lr_model), ('svm', svm_model), ('xgb', xgboost_model)]),
                                 soft_voting_params, cv=20) # Assuming soft_voting_params are defined elsewhere
    soft_voting_model.fit(X_train, y_train)
    soft_voting_test_score = soft_voting_model.best_estimator_.score(X_test, y_test)
    soft_voting_train_score = soft_voting_model.best_estimator_.score(X_train, y_train)
    soft_voting_cv_scores = cross_val_score(soft_voting_model.best_estimator_, X_test, y_test, cv=20)
    avg_soft_voting_cv_score = np.mean(soft_voting_cv_scores)

    print(f"K={k}, Hard Voting Training Accuracy: {hard_voting_train_score}")
    print(f"K={k}, Hard Voting Testing Accuracy: {hard_voting_test_score}")

    print(f"K={k}, Soft Voting Training Accuracy: {soft_voting_train_score}")
    print(f"K={k}, Soft Voting Testing Accuracy: {soft_voting_test_score}")

    print(f"K={k}, Average Hard Voting Cross-Validation Score: {avg_hard_voting_cv_score}")
    print(f"K={k}, Average Soft Voting Cross-Validation Score: {avg_soft_voting_cv_score}")

    print(f"K={k}, Average Voting Cross-Validation Score: {(avg_hard_voting_cv_score + avg_soft_voting_cv_score) / 2}\n")


    if avg_hard_voting_cv_score > best_avg_cv_accuracy:
        best_avg_cv_accuracy = avg_hard_voting_cv_score
        best_k_features = k


print(f"\nBest K for Feature Selection: {best_k_features}")
print(f"Best Average Cross-Validation Accuracy: {best_avg_cv_accuracy}")

# prompt: perform rfe for all the feature and print test accuracy and avg hard and soft X Y cv for all feature

import numpy as np
from sklearn.feature_selection import RFE

# Assuming X and y are already defined

# Create a dictionary to store the model names and their cross-validation scores
model_scores = {
    "Random Forest": rf_avg_acc,
    "K-Nearest Neighbors": knn_avg_acc,
    "Logistic Regression": lr_avg_acc,
    "Support Vector Machine": svm_avg_acc
}

# Find the model with the highest cross-validation score
best_model_name = max(model_scores, key=model_scores.get)
if best_model_name == "Random Forest":
  best_model = rf_model
elif best_model_name == "K-Nearest Neighbors":
  best_model = knn_model
elif best_model_name == "Logistic Regression":
  best_model = lr_model
else:
  best_model = svm_model

# RFE with the best model
rfe = RFE(estimator=best_model, n_features_to_select=X.shape[1])
X_rfe = rfe.fit_transform(X, y)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.2, random_state=best_random_state)


# Evaluate the best model on the test set
y_test_pred = clf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Set Accuracy with RFE: {test_accuracy:.4f}")
print(f"clf: {clf}")

# Hard Voting
hard_voting_cv_scores = cross_val_score(hard_voting_model.best_estimator_, X, y, cv=20)
avg_hard_voting_cv_score = np.mean(hard_voting_cv_scores)


#Soft Voting
soft_voting_cv_scores = cross_val_score(soft_voting_model.best_estimator_, X, y, cv=20)
avg_soft_voting_cv_score = np.mean(soft_voting_cv_scores)

print(f"svm accuracy: {svm_avg_acc}")
print(f"knn accuracy: {knn_avg_acc}")
print(f"lr accuracy: {lr_avg_acc}")
print(f"rf accuracy: {rf_avg_acc}")

print(f"Average Hard Voting Cross-Validation Accuracy with RFE: {avg_hard_voting_cv_score}")
print(f"Average Soft Voting Cross-Validation Accuracy with RFE: {avg_soft_voting_cv_score}")

y_train_pred = clf.predict(X_train)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Train Set Accuracy with RFE: {train_accuracy:.4f}")

# prompt: loop trhough max num of feature with both soft and hrad to find best rfe and print training, Testing and CV

best_k_features = 0
best_avg_cv_accuracy = 0

for k in range(1, X.shape[1] + 1):
    # Apply feature selection using RFE with Logistic Regression
    rfe = RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=k)
    X_rfe = rfe.fit_transform(X, y)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.2)

    # Hard Voting
    hard_voting_model = GridSearchCV(VotingClassifier(estimators=[('rf', rf_model), ('knn', knn_model), ('lr', lr_model), ('svm', svm_model), ('xgb', xgboost_model)]),
                                 hard_voting_params, cv=20)
    hard_voting_model.fit(X_train, y_train)
    hard_voting_test_score = hard_voting_model.best_estimator_.score(X_test, y_test)
    hard_voting_train_score = hard_voting_model.best_estimator_.score(X_train, y_train)
    hard_voting_cv_scores = cross_val_score(hard_voting_model.best_estimator_, X_test, y_test, cv=20)
    avg_hard_voting_cv_score = np.mean(hard_voting_cv_scores)

    #Soft Voting
    soft_voting_model = GridSearchCV(VotingClassifier(estimators=[('rf', rf_model), ('knn', knn_model), ('lr', lr_model), ('svm', svm_model), ('xgb', xgboost_model)]),
                                 soft_voting_params, cv=20)
    soft_voting_model.fit(X_train, y_train)
    soft_voting_test_score = soft_voting_model.best_estimator_.score(X_test, y_test)
    soft_voting_train_score = soft_voting_model.best_estimator_.score(X_train, y_train)
    soft_voting_cv_scores = cross_val_score(soft_voting_model.best_estimator_, X_test, y_test, cv=20)
    avg_soft_voting_cv_score = np.mean(soft_voting_cv_scores)


    print(f"K={k}, Hard Voting Training Accuracy: {hard_voting_train_score}")
    print(f"K={k}, Hard Voting Testing Accuracy: {hard_voting_test_score}")
    print(f"K={k}, Soft Voting Training Accuracy: {soft_voting_train_score}")
    print(f"K={k}, Soft Voting Testing Accuracy: {soft_voting_test_score}")
    print(f"K={k}, Average Hard Voting Cross-Validation Score: {avg_hard_voting_cv_score}")
    print(f"K={k}, Average Soft Voting Cross-Validation Score: {avg_soft_voting_cv_score}")
    print(f"K={k}, Average Voting Cross-Validation Score: {(avg_hard_voting_cv_score + avg_soft_voting_cv_score) / 2}\n")

    if (avg_hard_voting_cv_score + avg_soft_voting_cv_score) / 2 > best_avg_cv_accuracy:
        best_avg_cv_accuracy = (avg_hard_voting_cv_score + avg_soft_voting_cv_score) / 2
        best_k_features = k


print(f"\nBest K for Feature Selection (RFE): {best_k_features}")
print(f"Best Average Cross-Validation Accuracy (RFE): {best_avg_cv_accuracy}")

