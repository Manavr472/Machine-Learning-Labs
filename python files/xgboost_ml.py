# -*- coding: utf-8 -*-
"""XGBoost ML lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A7HSQCd5sLrajO70YvCglhUyVzYOi-4Z
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean.csv')

data.info()

columns_to_drop = [
    'drugbank-id','type', 'created', 'updated', 'description', 'cas-number', 'unii', 'food-interactions',
    'synthesis-reference', 'indication', 'pharmacodynamics', 'mechanism-of-action',
    'toxicity', 'metabolism', 'absorption', 'half-life', 'protein-binding',
    'route-of-elimination', 'volume-of-distribution', 'clearance', 'ahfs-codes',
    'sequences', 'pathways', 'reactions', 'snp-effects', 'snp-adverse-drug-reactions',
    'carriers', 'transporters', 'fda-label', 'msds', 'atc-codes',	'pdb-entries'
]

# Assuming 'drugbank-id', 'name', 'groups', 'pdb-entries', 'drug-interactions', 'targets', 'average-mass', 'monoisotopic-mass' are kept for your analysis.

# Drop columns from your dataset
data = data.drop(columns=columns_to_drop)

data.info()

data.head(10)

data.describe()

data['targets'] = data['targets'].apply(lambda x: 0 if pd.isnull(x) else 1)

data.head(10)

data.dropna(subset=['name'], inplace=True)

data.dropna(subset=['drug-interactions'], inplace=True)

data.dropna(subset=['state'], inplace=True)

data.dropna(subset=['enzymes'], inplace=True)

data.info()

# prompt: do label encoding

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to categorical columns
categorical_cols = ['name','state','groups','drug-interactions','enzymes']  # Replace with actual categorical column names
for col in categorical_cols:
  data[col] = label_encoder.fit_transform(data[col])

# Display the modified DataFrame
data.head()

data.isnull()

data.info()

average_mass_median = data['average-mass'].median()
monoisotopic_mass_median = data['monoisotopic-mass'].median()
print("Average Mass Median:", average_mass_median)
print("Monoisotopic Mass Median:", monoisotopic_mass_median)
# Fill null values with median
data['average-mass'].fillna(average_mass_median, inplace=True)
data['monoisotopic-mass'].fillna(monoisotopic_mass_median, inplace=True)

data.info()

data.isnull()

data.describe()

# prompt: make scartter plot for average-mass and monoisotopic-mas different plot for both

import matplotlib.pyplot as plt
# Scatter plot for 'average-mass'
plt.figure(figsize=(8, 6))
sns.scatterplot(x=data.index, y=data['average-mass'])
plt.title('Scatter Plot of Average Mass')
plt.xlabel('Index')
plt.ylabel('Average Mass')
plt.show()

# Scatter plot for 'monoisotopic-mass'
plt.figure(figsize=(8, 6))
sns.scatterplot(x=data.index, y=data['monoisotopic-mass'])
plt.title('Scatter Plot of Monoisotopic Mass')
plt.xlabel('Index')
plt.ylabel('Monoisotopic Mass')
plt.show()

# prompt: plot box plot to track outliers for all columns

import matplotlib.pyplot as plt
# Box plots for numerical columns to detect outliers
numerical_cols = ['average-mass', 'monoisotopic-mass']
for col in numerical_cols:
  plt.figure()
  sns.boxplot(x=data[col])
  plt.title(f'Box Plot of {col}')
  plt.show()

# prompt: use z-score to eliminate outliers\

import numpy as np
from scipy import stats

# Calculate z-scores for numerical columns
numerical_cols = ['average-mass', 'monoisotopic-mass']
z_scores = stats.zscore(data[numerical_cols])

# Set a threshold for outlier detection (e.g., 3 standard deviations)
threshold = 3

# Filter data to remove outliers
data_filtered = data[(np.abs(z_scores) < threshold).all(axis=1)]

# Display the shape of the filtered DataFrame
print("Shape of filtered data:", data_filtered.shape)

# prompt: plot box plot to track outliers for all columns

import matplotlib.pyplot as plt
# Box plots for numerical columns to detect outliers
numerical_cols = ['average-mass', 'monoisotopic-mass']
for col in numerical_cols:
  plt.figure()
  sns.boxplot(x=data_filtered[col])
  plt.title(f'Box Plot of {col}')
  plt.show()

data.describe()

# prompt: export cleaned data as csv file

# Assuming 'data_filtered_ior' is your final cleaned DataFrame
data_filtered.to_csv('cleaned_drug_data(z_score).csv', index=False)

# prompt: apply xgboost on both z scroe data

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

# Assuming 'targets' is your target variable and the rest are features
X = data_filtered.drop('targets', axis=1)
y = data_filtered['targets']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Initialize XGBoost classifier
model = xgb.XGBClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)
y_train_pred = model.predict(X_train)

# Evaluate the model
print("Training Accuracy:", accuracy_score(y_train, y_train_pred))
print("Testing Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=20)  # 5-fold cross-validation
print("\nCross-Validation Scores:", cv_scores)
print("Average CV Score:", cv_scores.mean())

import matplotlib.pyplot as plt
# Assuming 'cv_scores' contains your cross-validation scores
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='--')
plt.title('Cross-Validation Scores')
plt.xlabel('Fold')
plt.ylabel('Score')
plt.grid(True)
plt.show()

# prompt: create a loop to get the best random state performance in svm

best_accuracy = 0
best_random_state = None

for random_state in range(0, 5000):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)
  model = xgb.XGBClassifier()
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)

  if accuracy > best_accuracy:
    best_accuracy = accuracy
    best_random_state = random_state

print("Best Random State:", best_random_state)
print("Best Accuracy:", best_accuracy)

# prompt: run the for loop for k for 50 times and select K with most repetetion

from sklearn.feature_selection import chi2, SelectKBest

k_counts = {}
for _ in range(1):
    best_accuracy = 0
    best_k = 0

    for k in range(1, 8):
        # Feature selection
        selector = SelectKBest(score_func=chi2, k=k)
        X_new = selector.fit_transform(X, y)

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2)

        # Initialize XGBoost classifier
        xgb_classifier = xgb.XGBClassifier()

        # Train the classifier
        xgb_classifier.fit(X_train, y_train)

        # Make predictions on the test set
        y_pred = xgb_classifier.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred)

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_k = k

    # Update k counts
    if best_k in k_counts:
        k_counts[best_k] += 1
    else:
        k_counts[best_k] = 1

# Find the most frequent k
most_frequent_k = max(k_counts, key=k_counts.get)

print(f"Most frequently selected k: {most_frequent_k}")

# select above Fetures in X and perform svm

k = most_frequent_k

# Feature selection with the selected k
selector = SelectKBest(score_func=chi2, k=k)
X_selected = selector.fit_transform(X, y)

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = X.columns[selected_feature_indices]

print(f"Selected Features for k={k}: {selected_feature_names}")

# Split the data into training and testing sets
X_train_selected, X_test_selected, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)

# Initialize XGBoost classifier
xgb_classifier_selected = xgb.XGBClassifier()

# Train the classifier on the selected features
xgb_classifier_selected.fit(X_train_selected, y_train)

# Make predictions on the test set
y_pred_selected = xgb_classifier_selected.predict(X_test_selected)
y_train_pred_selected = xgb_classifier_selected.predict(X_train_selected)

# Evaluate the model
print("Training Accuracy (Selected Features):", accuracy_score(y_train, y_train_pred_selected))
print("Testing Accuracy (Selected Features):", accuracy_score(y_test, y_pred_selected))
print("\nClassification Report (Selected Features):\n", classification_report(y_test, y_pred_selected))
print("\nConfusion Matrix (Selected Features):\n", confusion_matrix(y_test, y_pred_selected))

new_cv_scores = cross_val_score(xgb_classifier_selected, X_selected, y, cv=20)  # 5-fold cross-validation
print("\nCross-Validation Scores:", new_cv_scores)
print("Average CV Score:", new_cv_scores.mean())

import matplotlib.pyplot as plt
# Assuming 'cv_scores' contains your cross-validation scores
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(new_cv_scores) + 1), new_cv_scores, marker='o', linestyle='--')
plt.title('Cross-Validation Scores')
plt.xlabel('Fold')
plt.ylabel('Score')
plt.grid(True)
plt.show()

from sklearn.feature_selection import RFE

num_features = []
accuracies = []

for k in range(1, 8):
    # Initialize XGBoost classifier with a linear kernel
    estimator = xgb.XGBClassifier()

    # Initialize RFE with the estimator and desired number of features
    selector = RFE(estimator, n_features_to_select=k)

    # Fit RFE to the data
    selector = selector.fit(X, y)

    # Get the selected features
    X_selected = X.iloc[:, selector.support_]

    # Split the data into training and testing sets
    X_train_selected, X_test_selected, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)

    # Train the XGBoost classifier on the selected features
    estimator.fit(X_train_selected, y_train)

    # Make predictions on the test set
    y_pred_selected = estimator.predict(X_test_selected)
    y_train_pred_selected = estimator.predict(X_train_selected)

    # Evaluate the model
    train_accuracy = accuracy_score(y_train, y_train_pred_selected)
    accuracy = accuracy_score(y_test, y_pred_selected)


    # Store results
    num_features.append(k)
    accuracies.append(accuracy)

    print(f"Training Accuracy with {k} features (RFE): {train_accuracy}")
    print(f"Testing Accuracy with {k} features (RFE): {accuracy}")

# Find the best number of features
best_k = num_features[np.argmax(accuracies)]
best_accuracy = np.max(accuracies)

print(f"\nBest accuracy: {best_accuracy} achieved with {best_k} features (RFE)")

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_feature_names = X.columns[selected_feature_indices]

print("Selected Features for Best RFE:", selected_feature_names)

# Plot the results
plt.figure(figsize=(8, 6))
plt.plot(num_features, accuracies, marker='o', linestyle='--')
plt.title('Accuracy vs. Number of Features (RFE)')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

new_cv_scores = cross_val_score(estimator, X_train_selected, y_train, cv=20)  # 5-fold cross-validation
print("\nCross-Validation Scores:", new_cv_scores)
print("Average CV Score:", new_cv_scores.mean())

import matplotlib.pyplot as plt
# Assuming 'cv_scores' contains your cross-validation scores
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(new_cv_scores) + 1), new_cv_scores, marker='o', linestyle='--')
plt.title('Cross-Validation Scores')
plt.xlabel('Fold')
plt.ylabel('Score')
plt.grid(True)
plt.show()

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# Define the parameter grid to search for XGBoost
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'gamma': [0, 0.1, 0.3],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Initialize GridSearchCV with XGBoost classifier and parameter grid
grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=20, scoring='f1')

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and best estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

print("Best Parameters:", best_params)

# Make predictions on the test set using the best estimator
y_pred_best = best_estimator.predict(X_test)

# Evaluate the model with the best parameters
print("Accuracy (Best Parameters):", accuracy_score(y_test, y_pred_best))
print("\nClassification Report (Best Parameters):\n", classification_report(y_test, y_pred_best))
print("\nConfusion Matrix (Best Parameters):\n", confusion_matrix(y_test, y_pred_best))

