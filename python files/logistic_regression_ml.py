# -*- coding: utf-8 -*-
"""Logistic Regression ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17TE0_ZJ-qQwFBc32IfCDh4V6uPNE1K_p
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean.csv')

data.info()

columns_to_drop = [
    'drugbank-id','type', 'created', 'updated', 'description', 'cas-number', 'unii', 'food-interactions',
    'synthesis-reference', 'indication', 'pharmacodynamics', 'mechanism-of-action',
    'toxicity', 'metabolism', 'absorption', 'half-life', 'protein-binding',
    'route-of-elimination', 'volume-of-distribution', 'clearance', 'ahfs-codes',
    'sequences', 'pathways', 'reactions', 'snp-effects', 'snp-adverse-drug-reactions',
    'carriers', 'transporters', 'fda-label', 'msds', 'atc-codes',	'pdb-entries'
]

# Assuming 'drugbank-id', 'name', 'groups', 'pdb-entries', 'drug-interactions', 'targets', 'average-mass', 'monoisotopic-mass' are kept for your analysis.

# Drop columns from your dataset
data = data.drop(columns=columns_to_drop)

data.info()

data.head(10)

data.describe()

data['targets'] = data['targets'].apply(lambda x: 0 if pd.isnull(x) else 1)

data.head(10)

data.dropna(subset=['name'], inplace=True)

data.dropna(subset=['drug-interactions'], inplace=True)

data.dropna(subset=['state'], inplace=True)

data.dropna(subset=['enzymes'], inplace=True)

data.info()

# prompt: do label encoding

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to categorical columns
categorical_cols = ['name','state','groups','drug-interactions','enzymes']  # Replace with actual categorical column names
for col in categorical_cols:
  data[col] = label_encoder.fit_transform(data[col])

# Display the modified DataFrame
data.head()

data.isnull()

data.info()

average_mass_median = data['average-mass'].median()
monoisotopic_mass_median = data['monoisotopic-mass'].median()
print("Average Mass Median:", average_mass_median)
print("Monoisotopic Mass Median:", monoisotopic_mass_median)
# Fill null values with median
data['average-mass'].fillna(average_mass_median, inplace=True)
data['monoisotopic-mass'].fillna(monoisotopic_mass_median, inplace=True)

data.info()

data.isnull()

# prompt: perform IQR to remove outliers and generate a csv file

# Calculate IQR for relevant numerical columns
Q1 = data[['average-mass', 'monoisotopic-mass']].quantile(0.25)
Q3 = data[['average-mass', 'monoisotopic-mass']].quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter data to remove outliers
data_filtered = data[~((data[['average-mass', 'monoisotopic-mass']] < lower_bound) | (data[['average-mass', 'monoisotopic-mass']] > upper_bound)).any(axis=1)]

# Save filtered data to a CSV file
data_filtered.to_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean_no_outliers_IQR.csv', index=False)

import os

# ... (your existing code for calculating IQR and filtering data) ...

# Check if the directory exists, and create it if necessary
directory_path = '/content/drive/MyDrive/Machine learning Lab'  # Update with your directory path
if not os.path.exists(directory_path):
    os.makedirs(directory_path)
    print(f"Created directory: {directory_path}")
else:
    print(f"Directory already exists: {directory_path}")

# Save filtered data to a CSV file
data_filtered.to_csv(os.path.join(directory_path, 'drugbank_clean_no_outliers_IQR.csv'), index=False)

# prompt: perfoem logistic regression on this filetered data
import warnings
warnings.filterwarnings("ignore")
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, KFold
import matplotlib.pyplot as plt

# Load the filtered data
data_filtered = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean_no_outliers_IQR.csv')

# Assuming 'targets' is your target variable and the rest are features
X = data_filtered.drop('targets', axis=1)
y = data_filtered['targets']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Initialize the logistic regression model
model = LogisticRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
y_train_pred = model.predict(X_train)

# Evaluate the model
test_accuracy = accuracy_score(y_test, y_pred)
train_accuracy = accuracy_score(y_train, y_train_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Training Accuracy:", train_accuracy)
print("Test Accuracy:", test_accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

# Initialize KFold with 10 splits
kf = KFold(n_splits=20)

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Print cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Average cross-validation score:", cv_scores.mean())

# Visualize cross-validation scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, 21), cv_scores, marker='o', linestyle='--')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Scores')
plt.grid(True)
plt.show()

# prompt: perfoem logistic regression on this filetered data
warnings.filterwarnings("ignore")
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, KFold
import matplotlib.pyplot as plt

# Load the filtered data
data_filtered = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean_no_outliers_IQR.csv')

# Assuming 'targets' is your target variable and the rest are features
X = data_filtered.drop('targets', axis=1)
y = data_filtered['targets']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Initialize the logistic regression model
model = LogisticRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)
y_train_pred = model.predict(X_train)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

train_accuracy = accuracy_score(y_train, y_train_pred)
print("Training Accuracy:", train_accuracy)

print("Testing Accuracy:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

# Initialize KFold with 10 splits
kf = KFold(n_splits=20)

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Print cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Average cross-validation score:", cv_scores.mean())

# Visualize cross-validation scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, 21), cv_scores, marker='o', linestyle='--')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Scores')
plt.grid(True)
plt.show()

# prompt: use best k and do chi2

import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2

# Load the filtered data
data_filtered = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/cleaned_drug_data_z2.csv')

# Assuming 'targets' is your target variable and the rest are features
X = data_filtered.drop('targets', axis=1)
y = data_filtered['targets']

# Initialize variables to store the best K and its frequency
best_k = 0
max_frequency = 0
k_frequencies = {}

# Iterate through different K values
for _ in range(1):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  # Iterate through different numbers of features (K)
  for k in range(3, X_train.shape[1] + 1):
    # Feature selection using SelectKBest and chi2
    selector = SelectKBest(score_func=chi2, k=k)
    X_train_selected = selector.fit_transform(X_train, y_train)
    X_test_selected = selector.transform(X_test)

    # Train a logistic regression model
    model = LogisticRegression()
    model.fit(X_train_selected, y_train)

    # Make predictions and calculate accuracy
    y_pred = model.predict(X_test_selected)
    accuracy = accuracy_score(y_test, y_pred)

    # Perform cross-validation
    kf = KFold(n_splits=20)
    cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
    average_cv_score = cv_scores.mean()

    # Check if the current K has a higher frequency
    if k in k_frequencies:
      k_frequencies[k] += 1
    else:
      k_frequencies[k] = 1

    if k_frequencies[k] > max_frequency:
      max_frequency = k_frequencies[k]
      best_k = k

# Print the best K and its frequency
print("Best K:", best_k)
print("Frequency:", max_frequency)

# Perform feature selection with the selected K
selector = SelectKBest(score_func=chi2, k=best_k)
X_selected = selector.fit_transform(X, y)

# Split the data with selected features into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)

# Train the final model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions and calculate accuracy
y_pred = model.predict(X_test)
y_train_pred = model.predict(X_train)

train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_pred)

# Perform cross-validation
kf = KFold(n_splits=20)
cv_scores = cross_val_score(model, X_selected, y, cv=kf, scoring='accuracy')
average_cv_score = cv_scores.mean()

print("Training Accuracy:", train_accuracy)
print("Accuracy with best K:", test_accuracy)
print("Average Cross-validation score with best K:", average_cv_score)

# prompt: apply rfe loop through all the feature and show cv acc for all the features

import pandas as pd
from sklearn.feature_selection import RFE

# Load the filtered data
data_filtered = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/cleaned_drug_data_z2.csv')

# Assuming 'targets' is your target variable and the rest are features
X = data_filtered.drop('targets', axis=1)
y = data_filtered['targets']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# Initialize variables to store the best number of features and its CV accuracy
best_num_features = 0
best_cv_accuracy = 0

# Iterate through different numbers of features
for num_features in range(1, X.shape[1] + 1):
    # Initialize the logistic regression model
    model = LogisticRegression()

    # Initialize RFE with the current number of features
    rfe = RFE(estimator=model, n_features_to_select=num_features)

    # Fit RFE to the data
    X_rfe = rfe.fit_transform(X_train, y_train)

    # Perform cross-validation
    kf = KFold(n_splits=20)
    cv_scores = cross_val_score(model, X_rfe, y_train, cv=kf, scoring='accuracy')
    average_cv_accuracy = cv_scores.mean()

    # Print the CV accuracy for the current number of features
    print(f"Number of Features: {num_features}, Average CV Accuracy: {average_cv_accuracy}")

    # Update the best number of features and its CV accuracy if necessary
    if average_cv_accuracy > best_cv_accuracy:
        best_cv_accuracy = average_cv_accuracy
        best_num_features = num_features

# Print the best number of features and its CV accuracy
print(f"\nBest Number of Features: {best_num_features}, Best CV Accuracy: {best_cv_accuracy}")

# prompt: find training testing accuracy and CV with best RFE

# Load the filtered data
data_filtered = pd.read_csv('/content/drive/MyDrive/Machine learning Lab/drugbank_clean_no_outliers_IQR.csv')

# Assuming 'targets' is your target variable and the rest are features
X = data_filtered.drop('targets', axis=1)
y = data_filtered['targets']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Initialize variables to store the best number of features and its CV accuracy
best_num_features = 0
best_cv_accuracy = 0

# Iterate through different numbers of features
for num_features in range(1, X.shape[1] + 1):
    # Initialize the logistic regression model
    model = LogisticRegression()

    # Initialize RFE with the current number of features
    rfe = RFE(estimator=model, n_features_to_select=num_features)

    # Fit RFE to the data
    X_train_rfe = rfe.fit_transform(X_train, y_train)

    # Train the model with selected features
    model.fit(X_train_rfe, y_train)

    # Transform the test data with the selected features
    X_test_rfe = rfe.transform(X_test)

    # Predict on the test data
    y_pred = model.predict(X_test_rfe)

    # Calculate the test accuracy
    test_accuracy = accuracy_score(y_test, y_pred)


    # Perform cross-validation
    kf = KFold(n_splits=20)
    cv_scores = cross_val_score(model, X_train_rfe, y_train, cv=kf, scoring='accuracy')
    average_cv_accuracy = cv_scores.mean()

    # Print the CV accuracy for the current number of features
    print(f"Number of Features: {num_features}, Average CV Accuracy: {average_cv_accuracy}, Test Accuracy: {test_accuracy}")

    # Update the best number of features and its CV accuracy if necessary
    if average_cv_accuracy > best_cv_accuracy:
        best_cv_accuracy = average_cv_accuracy
        best_num_features = num_features

# Print the best number of features and its CV accuracy
print(f"\nBest Number of Features: {best_num_features}, Best CV Accuracy: {best_cv_accuracy}")

# Now, train the final model with the best number of features
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=best_num_features)
X_train_rfe = rfe.fit_transform(X_train, y_train)
model.fit(X_train_rfe, y_train)
X_test_rfe = rfe.transform(X_test)
y_pred = model.predict(X_test_rfe)

# Calculate and print the final test accuracy and training accuracy
test_accuracy = accuracy_score(y_test, y_pred)
y_train_pred = model.predict(X_train_rfe)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Final Test Accuracy with best RFE: {test_accuracy}")
print(f"Final Training Accuracy with best RFE: {train_accuracy}")

# Perform cross-validation with the final model
kf = KFold(n_splits=20)
cv_scores = cross_val_score(model, X_train_rfe, y_train, cv=kf, scoring='accuracy')
average_cv_accuracy = cv_scores.mean()
print(f"Final Average CV Accuracy with best RFE: {average_cv_accuracy}")

